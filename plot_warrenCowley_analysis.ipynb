{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e89a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "from ase.io import read,write\n",
    "from ovito.io import import_file\n",
    "import WarrenCowleyParameters as wc\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "def progress_print(i, total, comment=''):\n",
    "    \"\"\"\n",
    "    Print progress percentage and current file number being processed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    i : int\n",
    "        Current iteration index (0-based).\n",
    "    total : int\n",
    "        Total number of items to process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Prints progress to stdout with carriage return for dynamic updating.\n",
    "    \"\"\"\n",
    "    progress = (i + 1) / total * 100\n",
    "    print(f'{comment} Progress: {progress:.0f}% \\t Processing File {i+1}/{total}', end='\\r', flush=True)\n",
    "\n",
    "simulation_folder = '/nfshome/winkelmann/running_sims/with_perfect_starting_cells/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c76b75",
   "metadata": {},
   "source": [
    "# truncate to only Li and X Atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc28a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 ovito files to process.\n",
      "excluded 233 truncated files and their base files.\n",
      "excluded 55 user defined files from filelist.\n"
     ]
    }
   ],
   "source": [
    "#find Ovito files to process, excluding truncated files and files in the filelist\n",
    "filelist_to_exclude = os.path.join(simulation_folder, './filelist_failed.txt')\n",
    "\n",
    "# find all .XDATCAR files in all subfolders but not the base folder\n",
    "pattern = os.path.join(simulation_folder, '*', '**', '*.XDATCAR')\n",
    "path_all_ovito_files = sorted(glob(pattern, recursive=True))\n",
    "\n",
    "user_defined_files_to_exclude = []\n",
    "# exclude files on the basis of a filelist\n",
    "if os.path.exists(filelist_to_exclude):\n",
    "    with open(filelist_to_exclude, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                user_defined_files_to_exclude.append(os.path.realpath(line))\n",
    "else: user_defined_files_to_exclude = []  # If filelist does not exist, just use an empty list\n",
    "\n",
    "# Find all truncated files and create list of their original files\n",
    "path_truncated_files = [f for f in path_all_ovito_files if f.endswith('_truncated.XDATCAR')]\n",
    "#[f.replace('_truncated.XDATCAR', '.XDATCAR') for f in truncated_files]\n",
    "\n",
    "# Exclude both truncated files and their base files\n",
    "path_all_ovito_files = [f for f in path_all_ovito_files if f not in path_truncated_files and \n",
    "                        f not in user_defined_files_to_exclude and \n",
    "                        f not in [t.replace('_truncated.XDATCAR', '.XDATCAR') for t in path_truncated_files]]\n",
    "print(f\"Found {len(path_all_ovito_files)} ovito files to process.\")\n",
    "print(f'excluded {len(path_truncated_files)} truncated files and their base files.')\n",
    "print(f'excluded {len(user_defined_files_to_exclude)} user defined files from filelist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd85a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncate the files to just Li and X\n",
    "def truncate_ovito_files(path_ovito_file, wanted_species=['Li','X']):\n",
    "    with open(path_ovito_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    step, time = [], []\n",
    "    for line in lines:\n",
    "        if 'step' in line:\n",
    "            words = line.split(' ')\n",
    "            step.append(int(words[1]))\n",
    "            time.append(float(words[3]))\n",
    "    \n",
    "    all_frames = read(path_ovito_file, index=':')\n",
    "    truncated_ovito_file = []\n",
    "    for n,atoms in enumerate(all_frames):  \n",
    "        indices = [i for i, atom in enumerate(atoms) if atom.symbol in wanted_species]\n",
    "        new_atoms = atoms[indices]\n",
    "        new_atoms.info = atoms.info\n",
    "        new_atoms.write(filename=path_ovito_file.replace('.XDATCAR', f'_truncated.XDATCAR'),\n",
    "                        format='vasp-xdatcar', append=True, label=f'step {step[n]} time {time[n]}')\n",
    "        truncated_ovito_file.append(new_atoms)      \n",
    "    return truncated_ovito_file\n",
    "\n",
    "for i, path_ovito_file in enumerate(path_all_ovito_files):\n",
    "    progress_print(i, len(path_all_ovito_files))\n",
    "    truncate_ovito_files(path_ovito_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0f36e",
   "metadata": {},
   "source": [
    "# calculate Warren Cowley and safe it to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dec208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 truncated files to process.\n",
      "Skipped 0 files that already have Warren-Cowley CSV files.\n"
     ]
    }
   ],
   "source": [
    "# find all truncated files to process for Warren-Cowley analysis\n",
    "pattern = os.path.join(simulation_folder, '*', '**', '*_truncated.XDATCAR')\n",
    "path_truncated_files = sorted(glob(pattern, recursive=True))\n",
    "\n",
    "# Exclude files that already have a corresponding _warren_cowley.csv\n",
    "path_truncated_files = []\n",
    "path_existing_wc_files = [f for f in path_truncated_files if os.path.exists(f.replace('_truncated.XDATCAR', '_warren_cowley.csv'))]\n",
    "path_truncated_files = [f for f in path_truncated_files if f not in path_existing_wc_files]\n",
    "\n",
    "print(f\"Found {len(path_truncated_files)} truncated files to process.\")\n",
    "print(f\"Skipped {len(path_existing_wc_files)} files that already have Warren-Cowley CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e26c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_warren_cowley_csv(structure_file):\n",
    "    try:\n",
    "        with open(structure_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        step, time = [], []\n",
    "        for line in lines:\n",
    "            if 'step' in line:\n",
    "                words = line.split(' ')\n",
    "                step.append(int(words[1]))\n",
    "                time.append(float(words[3]))\n",
    "        ovito_file = import_file(structure_file)\n",
    "        num_frames = ovito_file.source.num_frames\n",
    "        if num_frames != len(step):\n",
    "            warnings.warn(f\"Number of frames does not match number of steps found. frames = {num_frames}, steps = {len(step)}\")\n",
    "        \n",
    "        mod = wc.WarrenCowleyParameters(nneigh=[0, 6])\n",
    "        ovito_file.modifiers.append(mod)\n",
    "        \n",
    "        data = ovito_file.compute(0)\n",
    "        wc_names = list(data.attributes['Warren-Cowley parameters by particle name'][0].keys())\n",
    "        \n",
    "        wc_file = structure_file.replace('Ovito_truncated.XDATCAR', '_warren_cowley.csv')\n",
    "        with open(wc_file, 'w') as f:\n",
    "            f.write(f\"{'step':<7},\\t{'time':<15},\\t{wc_names[0]:<20},\\t{wc_names[1]:<20},\\t{wc_names[2]:<20},\\t{wc_names[3]:<20}\\n\")\n",
    "        # Iterate through frames\n",
    "        for frame in range(num_frames):\n",
    "            data = ovito_file.compute(frame)\n",
    "            wc_for_shells = data.attributes['Warren-Cowley parameters'][0]\n",
    "            with open(wc_file, 'a') as f:\n",
    "                f.write(\n",
    "                    f\"{step[frame]:7},\\t{time[frame]:15},\\t{wc_for_shells[0][0]:20},\\t{wc_for_shells[0][1]:20},\\t\"\n",
    "                    f\"{wc_for_shells[1][0]:20},\\t{wc_for_shells[1][1]:20}\\n\"\n",
    "                )\n",
    "    except Warning as w:\n",
    "        warnings.warn(f\"While processing {structure_file}: {w}\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error while processing {structure_file}: {e}\")\n",
    "    \n",
    "\n",
    "for i, path in enumerate(path_truncated_files):\n",
    "    progress_print(i, len(path_truncated_files))\n",
    "    create_warren_cowley_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33e926",
   "metadata": {},
   "source": [
    "# Plot Warren Cowley files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1197524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_without_na(csv_file):\n",
    "    try:\n",
    "        # Read data\n",
    "        dataset = pd.read_csv(path, sep=r',\\s*', engine='python')\n",
    "        dataset.columns = dataset.columns.str.strip()\n",
    "        \n",
    "        # Remove rows with NaN values and keep track of dropped indices\n",
    "        original_indices = dataset.index\n",
    "        dataset.dropna(inplace=True)\n",
    "        dropped_indices = original_indices.difference(dataset.index)\n",
    "        if not dropped_indices.empty:\n",
    "            warnings.warn(f\"Dropped {len(dropped_indices)} rows with NaN values from {csv_file}. Dropped indices: {dropped_indices.tolist()}\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error reading {csv_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8047a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 233 warren cowley files to process.\n"
     ]
    }
   ],
   "source": [
    "pattern = os.path.join(simulation_folder, '*', '**', '*_warren_cowley.csv')\n",
    "path_wc_files = sorted(glob(pattern, recursive=True))\n",
    "\n",
    "paths_wc_files_per_concentration = {} \n",
    "for path in path_wc_files:\n",
    "    dir_name = os.path.dirname(os.path.dirname(path)).split('/')[-1]\n",
    "    if dir_name not in paths_wc_files_per_concentration.keys():\n",
    "        paths_wc_files_per_concentration[dir_name] = []\n",
    "    paths_wc_files_per_concentration[dir_name].append(path)\n",
    "print(f\"Found {len(path_wc_files)} warren cowley files to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7dbe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single wc plots Progress: 100% \t Processing File 233/233\r"
     ]
    }
   ],
   "source": [
    "# plots for each warren cowley file\n",
    "for i, path in enumerate(path_wc_files):\n",
    "    try:\n",
    "        progress_print(i, len(path_wc_files), 'single wc plots')\n",
    "        \n",
    "        # Read data\n",
    "        dataset = read_csv_without_na(path)\n",
    "\n",
    "        # Create figure\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Seaborn regression plot with scatter and confidence interval\n",
    "        # Using lowess=True for smooth non-linear curve (locally weighted regression)\n",
    "        sns.regplot(data=dataset, \n",
    "                    x='time', \n",
    "                    y='Li-X', \n",
    "                    scatter_kws={'alpha':0.5, 's':20},\n",
    "                    line_kws={'color':'red'},\n",
    "                    lowess=True)\n",
    "        \n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Li-X')\n",
    "        plt.title('Warren-Cowley Parameter: Li-X vs Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path.replace('.csv', '_Li-X_warren_cowley.png'))\n",
    "        plt.close()\n",
    "    except Warning as w:\n",
    "        warnings.warn(f\"while processing {path}: {w}\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error while processing {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots per concentration\n",
    "for i, (conc, paths) in enumerate(paths_wc_files_per_concentration.items()):\n",
    "    try:\n",
    "        progress_print(i, len(paths_wc_files_per_concentration), f'wc plots')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Combine all datasets from the cluster into one\n",
    "        collected_datasets = []\n",
    "        for run_num, path in enumerate(paths):\n",
    "            collected_dataset = read_csv_without_na(path)\n",
    "            collected_dataset['run'] = f'{run_num+1}'\n",
    "            collected_datasets.append(collected_dataset)\n",
    "        \n",
    "        # Concatenate all data into single dataframe\n",
    "        combined_dataset_per_conc = pd.concat(collected_datasets, ignore_index=True)\n",
    "\n",
    "        # Plot scatter with color by run\n",
    "        sns.scatterplot(data=combined_dataset_per_conc,\n",
    "                        x='time', \n",
    "                        y='Li-X', \n",
    "                        hue='run',  \n",
    "                        alpha=0.5, \n",
    "                        s=20)\n",
    "        \n",
    "        # Add single regression line (without hue)\n",
    "        sns.regplot(data=combined_dataset_per_conc, \n",
    "                    x='time', \n",
    "                    y='Li-X',\n",
    "                    scatter=False,\n",
    "                    #line_kws={'color':'red', 'linewidth':2},\n",
    "                    lowess=True)\n",
    "        plt.legend().remove()  # Remove legend for runs since it's to big\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Li-X')\n",
    "        plt.title(f'Warren-Cowley Parameter: Li-X vs Time for Li_{conc}%')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(simulation_folder, f'{conc}_warren_cowley.png'))\n",
    "        plt.close()\n",
    "    except Warning as w:\n",
    "        warnings.warn(f\"While processing {conc}: {w}\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error while processing {conc}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60ed55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one plot for all concentrations\n",
    "all_combined_datasets = []\n",
    "for conc, paths in paths_wc_files_per_concentration.items():\n",
    "    \n",
    "    # Combine all datasets from the cluster into one\n",
    "    collected_datasets = []\n",
    "    for run_num, path in enumerate(paths):\n",
    "        collected_dataset = read_csv_without_na(path)\n",
    "        collected_dataset['run'] = f'{run_num+1}'\n",
    "        collected_dataset['concentration'] = conc\n",
    "        collected_datasets.append(collected_dataset)    \n",
    "        \n",
    "    # Concatenate all data into single dataframe\n",
    "    combined_dataset_per_conc = pd.concat(collected_datasets, ignore_index=True)\n",
    "    all_combined_datasets.append(combined_dataset_per_conc)\n",
    "\n",
    "# Combine all concentrations\n",
    "final_dataset = pd.concat(all_combined_datasets, ignore_index=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Add single regression line (without hue)\n",
    "sns.lmplot(data=final_dataset,x='time', \n",
    "            y='Li-X',  \n",
    "            hue='concentration', \n",
    "            scatter=False, \n",
    "            lowess=True)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Li-X')\n",
    "plt.xlim(0, 1)\n",
    "plt.title(f'Warren-Cowley Parameter: Li-X vs Time for all Concentrations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(simulation_folder, f'Li_all_concentrations_warren_cowley.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548beec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
